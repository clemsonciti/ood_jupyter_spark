<%-
  # get _list_partitions
  p_cmd = "/opt/pbs/default/bin/qstat -Q | grep Rou | grep -v -E 'gpu|c1|c2|osg' | awk '{print $1}' | sort"
  begin
    output, status = Open3.capture2e(p_cmd)
    if status.success?
      _list_partitions = output.split("\n").map(&:strip).reject(&:blank?).sort
    else
      raise output
    end
  rescue => e
    _list_partitions = []
    p_error = e.message.strip
  end

-%>
---
cluster:
  - "palmetto"
  
form:
  - pbs_select
  - pbs_ncpus
  - pbs_mem
  - pbs_ngpus
  - pbs_gpu_model
  - pbs_interconnect
  - pbs_walltime
  - pbs_queue
  - spark_version
  - anaconda_version
  - num_workers
  - only_driver_on_root
  - bc_email_on_started

attributes:
  pbs_select:
    label: "Number of resource chunks (select)"
    widget: "select"
    value: "1"
    options:
      - ["1","1"]
      - ["2","2"]
      - ["4","4"]
      - ["8","8"]
      
  pbs_ncpus:
    label: "CPU cores per chunk (ncpus)"
    widget: "select"
    value: "1"
    options:
      - ["1","1"]
      - ["2","2"]
      - ["4","4"]
      - ["8","8"]
      - ["16","16"]
      - ["20","20"]
      - ["24","24"]
      - ["28","28"]
      - ["32 (bigmem queue only)","32"]
      - ["40","40"]
      - ["64 (bigmem queue only)","64"]
      - ["80 (bigmem queue only)","80"]
      
  pbs_mem:
    label: "Amount of memory per chunk (mem)"
    widget: "select"
    value: "1gb"
    options:
      - ["1gb","1gb"]
      - ["2gb","2gb"]
      - ["4gb","4gb"]
      - ["6gb","6gb"]
      - ["14gb","14gb"]
      - ["30gb","30gb"]
      - ["62gb","62gb"]
      - ["120gb","120gb"]
      - ["370gb","370gb"]
      - ["505gb (bigmem queue only)","505gb"]
      - ["750gb (bigmem queue only)","750gb"]
      - ["1000gb (bigmem queue only)","1000gb"]      
      - ["1500gb (bigmem queue only)","1500gb"]
      - ["2000gb (bigmem queue only)","2000gb"]

  pbs_ngpus:
    label: "Number of GPUs per chunk (ngpus)"
    widget: "select"
    value: ""
    options:
      - ["None",""]
      - ["1",":ngpus=1"]
      - ["2",":ngpus=2"]
      - ["4",":ngpus=4"]

  pbs_gpu_model:
    label: "GPU Model (gpu_model)"
    widget: "select"
    value: ""
    options:
      - ["None",""]
      - ["Any", ":gpu_model=any"]
      - ["K20",":gpu_model=k20"]
      - ["K40",":gpu_model=k40"]
      - ["P100",":gpu_model=p100"]
      - ["V100",":gpu_model=v100"]
      - ["V100 with NVLink",":gpu_model=v100nv"]

  pbs_interconnect:
    label: "Interconnect"
    widget: "select"
    value: "any"
    options:
      - ["any","any"]
      - ["1g - Ethernet older phases 1-6","1g"]
      - ["1ge - Ethernet phase 0 (bigmem queue","1ge"]
      - ["10g - Ethernet phase 7-19 and phase 0 (bigmem queue)","10ge"]
      - ["25g - Ethernet phase 7-19","25ge"]
      - ["56g - Ethernet phase 7-19","56g"]
      - ["100g - Ethernet phase 18 and above","100g"]

  pbs_walltime:
    label: "Walltime"
    widget: "select"
    value: "00:30:00"
    options:
      - ["30 minutes","00:30:00"]
      - ["1 hours","01:00:00"]
      - ["2 hours","02:00:00"]
      - ["4 hours","04:00:00"]
      - ["16 hours","16:00:00"]
      - ["24 hours","24:00:00"]
      - ["72 hours","72:00:00"]

  pbs_queue:
    label: "Queue"
    required: true
    value: "work1"
    help: |
      <small>Queue to submit the job to</small>
    <%- if p_error -%>
      <span class="text-danger">Error when parsing queues:</span>
      ```
      <%= p_error.gsub("\n", "\n      ") %>
      ```
    <%- end -%>
    <%- if _list_partitions.blank? -%>
    widget: text_field
    <%- else -%>
    widget: select
    options:
      <%- _list_partitions.each do |q| -%>
      - [ "<%= q %>", "<%= q %>" ]
      <%- end -%>
    <%- end -%>

  spark_version:
    widget: select
    label: "Spark version"
    options:
      - "2.4.8"
      - "3.1.2"
      
  anaconda_version:
    widget: select
    label: "Anaconda 3 version"
    options:
      - "2019.10"
      - "2020.07"
      - "2021.05"
  
  num_workers:
    widget: "number_field"
    value: "1"
    min: "1"
    max: "48"
    label: "Number of workers per node"
    help: |
      This describes how the cores and memory are divvied up on the node
      (*useful to reduce memory allocated for each worker*). Should be a
      natural factor of the number of cores on the node you chose above. Do **NOT**
      exceed the number of cores on the node.

  only_driver_on_root:
    widget: "check_box"
    label: Only launch the driver on the master node.
    help: |
      This is typically used for `.collect` and `.take` operations that require
      a large amount of memory allocated (> 2GB) for the driver process.
